\title{The Backpropagation Algorithm}
\author{
        John Purcell
}
\date{\today}

\documentclass[12pt]{article}

\begin{document}
\maketitle

\begin{abstract}
Finding weight gradients via calculus.
\end{abstract}

\section{Notation}

First, let's describe our notation for our neural network nodes, weights, biases and other quantities.

The activation (output) of a neuron will be referred to via the lowercase letter \textit{a}. We will refer to weights via lowercase \textit{w}'s. Similarly, biases will be referred to by lowercase \textit{b}'s.

Our neural network consists of a number of layers, each layer containing multiple neurons. We will number the layers relative to some arbitrary layer \textbf{L}. So, if we want to refer to the next layer after \textbf{L} (the layer that takes \textbf{L}'s output as its input), we refer to layer \textbf{L+1}. The previous layer supplies layer \textbf{L}'s input, and we will refer to that as layer \textbf{L-1}.

Quantities in the layer \textbf{L} will be denoted using an uppercase superscript. 

The vector of all 


\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
This is never printed